# ğŸš€ GuÃ­a para Mejorar Accuracy del Modelo

## Problemas Identificados

1. **ImÃ¡genes muy pequeÃ±as (160Ã—160)**: Pierde detalles importantes
2. **Modelo muy pequeÃ±o (B0)**: Capacidad limitada
3. **Augmentations muy simples**: No aprovecha bien los datos
4. **Head muy pequeÃ±o**: Insuficiente para aprender patrones complejos
5. **Pocas Ã©pocas**: No converge completamente
6. **Sin fine-tuning estructurado**: Entrena todo de una vez

## âœ… Soluciones Implementadas

### 1. ConfiguraciÃ³n Optimizada (`config_high_accuracy.yaml`)

- **Imagen**: 256Ã—256 (vs 160Ã—160) - **+60% mÃ¡s informaciÃ³n**
- **Modelo**: EfficientNet-B2 (vs B0) - **+70% mÃ¡s parÃ¡metros**
- **Head**: 512â†’256 (vs 256â†’128) - **+4x mÃ¡s capacidad**
- **Ã‰pocas**: 100 (vs 30) - **Convergencia completa**
- **LR**: 0.0003 con schedule cuidadoso
- **Augmentations**: Completos y estratÃ©gicos

### 2. Script de Entrenamiento Mejorado (`train_improved.py`)

**Fine-tuning en DOS ETAPAS**:

#### Etapa 1: Entrenar solo Head (20 Ã©pocas)
- Backbone **congelado** (pre-entrenado ImageNet)
- Solo entrena el clasificador personalizado
- Learning rate mÃ¡s alto (2x)
- **Ventaja**: Aprende rÃ¡pidamente patrones especÃ­ficos

#### Etapa 2: Fine-tuning completo (80 Ã©pocas)
- Todo el modelo **descongelado**
- Learning rate mÃ¡s bajo
- Ajusta features del backbone para el dominio especÃ­fico
- **Ventaja**: OptimizaciÃ³n completa del modelo

### 3. Augmentations Mejorados

**Antes** (simples):
- Flip horizontal
- Brightness/Contrast

**Ahora** (completos):
- âœ… RotaciÃ³n Â±30Â°
- âœ… Flip horizontal + vertical
- âœ… Brightness/Contrast/Saturation/Hue
- âœ… Affine transformations (translate, scale)
- âœ… Noise/Blur (simula condiciones reales)
- âœ… CoarseDropout (regularizaciÃ³n)

### 4. TÃ©cnicas Adicionales

- **Gradient Clipping**: Evita gradientes explosivos
- **Class Weights**: Balancea clases desbalanceadas
- **Label Smoothing**: Reduce overconfidence
- **Cosine Annealing**: Schedule suave de LR

## ğŸ“‹ CÃ³mo Usar en Google Colab

### Paso 1: Subir archivos necesarios

```python
# En Colab, sube estos archivos:
# - configs/config_high_accuracy.yaml
# - scripts/train_improved.py
# - src/data/dataset.py (actualizado)
# - src/models/models.py
# - data/splits/ (todos los archivos .txt)
```

### Paso 2: Instalar dependencias

```python
!pip install torch torchvision albumentations scikit-learn matplotlib seaborn pyyaml tqdm
```

### Paso 3: Montar dataset

```python
# OpciÃ³n A: Subir dataset a Colab
from google.colab import files
# Sube Data_Esp_Pic.zip y descomprime

# OpciÃ³n B: Desde Google Drive
from google.colab import drive
drive.mount('/content/drive')
# Copia dataset a /content/
```

### Paso 4: Ajustar rutas en config

Edita `config_high_accuracy.yaml`:
```yaml
data:
  source_dir: "/content/Data_Esp_Pic"  # Ruta en Colab
  splits_dir: "data/splits"
```

### Paso 5: Ejecutar entrenamiento

```python
!python scripts/train_improved.py
```

## ğŸ¯ Resultados Esperados

Con estas mejoras deberÃ­as ver:

- **Accuracy en validaciÃ³n**: 75-85% (vs 50-60% anterior)
- **Accuracy en test**: 70-80%
- **Top-3 Accuracy**: 85-95%
- **Tiempo de entrenamiento**: ~2-3 horas en GPU Colab

## ğŸ”§ Ajustes Adicionales (si aÃºn no es suficiente)

### Si accuracy sigue baja:

1. **Aumentar tamaÃ±o de imagen**:
   ```yaml
   image_size: 320  # En lugar de 256
   ```

2. **Usar modelo mÃ¡s grande**:
   ```yaml
   architecture: "efficientnet_b3"  # En lugar de b2
   ```

3. **MÃ¡s Ã©pocas**:
   ```yaml
   epochs: 150
   early_stopping_patience: 25
   ```

4. **Reducir label smoothing**:
   ```yaml
   label_smoothing: 0.0  # Sin smoothing
   ```

5. **Aumentar batch size** (si GPU lo permite):
   ```yaml
   batch_size: 64  # MÃ¡s estable
   ```

### Si hay overfitting:

1. **Aumentar dropout**:
   ```yaml
   dropout_rate_1: 0.5
   dropout_rate_2: 0.4
   ```

2. **MÃ¡s augmentations**:
   - Aumentar probabilidad de augmentations
   - Agregar mÃ¡s variaciones

3. **MÃ¡s weight decay**:
   ```yaml
   weight_decay: 0.0005
   ```

## ğŸ“Š Monitoreo

Durante el entrenamiento, observa:

- **Gap Train-Val**: Si es >10%, hay overfitting
- **Val accuracy estancada**: Puede necesitar mÃ¡s Ã©pocas o LR diferente
- **Loss no baja**: LR puede estar muy bajo

## âœ… Checklist de Mejora

- [ ] Usar `config_high_accuracy.yaml`
- [ ] Usar `train_improved.py` (fine-tuning en 2 etapas)
- [ ] Verificar que augmentations completos estÃ©n activos
- [ ] Entrenar mÃ­nimo 50-100 Ã©pocas
- [ ] Monitorear mÃ©tricas durante entrenamiento
- [ ] Evaluar en test set al final
- [ ] Revisar matriz de confusiÃ³n para identificar clases problemÃ¡ticas

## ğŸ“ Notas Finales

- **Paciencia**: El entrenamiento puede tardar 2-3 horas, pero vale la pena
- **GPU es esencial**: En CPU tomarÃ­a dÃ­as
- **Experimenta**: Prueba diferentes configuraciones
- **Documenta**: Guarda resultados de cada experimento

Â¡Buena suerte con el entrenamiento mejorado! ğŸš€

